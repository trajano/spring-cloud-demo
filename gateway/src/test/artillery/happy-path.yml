config:
  target: "http://localhost:28082"
  plugins:
    expect:
      outputFormat: prettyError
  phases:
    - duration: 1
      # arrivalCount:
      # arrivalCount: 20
      # This should equal the number of scenarios
      arrivalRate: 1
#      maxVusers: 2
  http:
    timeout: 5
  environments:
    localhost-direct-small-load:
      target: "http://localhost:28082"
      phases:
        - duration: 10
          arrivalRate: 20
    api-small-load:
      target: "https://api.trajano.net"
      phases:
        - duration: 10
          arrivalRate: 20
    localhost-direct-medium-load:
      target: "http://localhost:28082"
      phases:
        - duration: 60
          arrivalRate: 60
    localhost-direct-heavy-load:
      target: "http://localhost:28082"
      phases:
        - name: warmup
          duration: 10
          arrivalRate: 1
        - pause: 10
        - name: load
          duration: 30
          arrivalRate: 65
          maxVusers: 750
        - pause: 10
        - name: next-load
          duration: 40
          arrivalRate: 65
          maxVusers: 750
    localhost-traefik-ludicrous-load:
      target: "http://localhost:28080"
      phases:
        - name: warmup
          duration: 10
          arrivalRate: 1
        - name: load
          duration: 120
          arrivalRate: 65
          # From a single machine generating 4000 connections can cause EADDRINUSE in Windows https://github.com/artilleryio/artillery/issues/1038
          # that's both sides, so that sets a theoretical limit of 2000 connections assuming that is the only thing the machine is doing.
          # For traefik load there's client <-> traefik + traefik <-> gateway + gateway <-> service.  That's 6 connections per virtual user request
          # Let's assume there's 1000 connections used already by Windows and other services.  That leaves us 3000 connections
          # 3000 / 6 = 500
          maxVusers: 500
    localhost-direct-ludicrous-load:
      target: "http://localhost:28082"
      phases:
        - name: warmup
          duration: 10
          arrivalRate: 1
        - name: load
          duration: 100
          arrivalRate: 100
          # From a single machine generating 4000 connections can cause EADDRINUSE in Windows https://github.com/artilleryio/artillery/issues/1038
          # that's both sides, so that sets a theoretical limit of 2000 connections assuming that is the only thing the machine is doing.
          # For direct load there's client <-> gateway + gateway <-> service.  That's 4 connections per virtual user request
          # Let's assume there's 1000 connections used already by Windows and other services.  That leaves us 3000 connections
          # 3000 / 4 = 750
          # However, there's still the notion of 120 second default for TcpTimedWaitDelay which means unless the thread is disconnected for 120 seconds
          # it will be part of the limit
          maxVusers: 750
    api-sane-load:
      target: "https://m0.pnb.devhaus.net"
      phases:
        - name: sane-load
          duration: 120
          arrivalRate: 120
#          arrivalRate: 60
      http:
        timeout: 3
    api-heavy-load:
      target: "https://m0.lj-ppe.devhaus.net"
      phases:
        - name: warmup
          duration: 10
          arrivalRate: 1
        - pause: 10
        - name: load
          duration: 30
          arrivalRate: 65
          maxVusers: 750
        - pause: 10
        - name: next-load
          duration: 40
          arrivalRate: 65
          maxVusers: 750
    api-ludicrous-load:
      target: "https://m0.lj-ppe.devhaus.net"
      phases:
        - name: warmup
          duration: 10
          arrivalRate: 1
        - name: load
          duration: 100
          arrivalRate: 100
          # From a single machine generating 4000 connections can cause EADDRINUSE in Windows https://github.com/artilleryio/artillery/issues/1038
          # that's both sides, so that sets a theoretical limit of 2000 connections assuming that is the only thing the machine is doing.
          # For direct load there's client <-> gateway + gateway <-> service.  That's 4 connections per virtual user request
          # Let's assume there's 1000 connections used already by Windows and other services.  That leaves us 3000 connections
          # 3000 / 4 = 750
          # However, there's still the notion of 120 second default for TcpTimedWaitDelay which means unless the thread is disconnected for 120 seconds
          # it will be part of the limit
          maxVusers: 750
#after:
#  flow:
#    # This will allow the last connection to terminate and clean up from the TcpTimedWaitDelay
#    - think: 60
scenarios:
  - name: Happy path
    weight: 1
    flow:
      - get:
          url: "/jwks"
          headers:
            Accept: application/json
          expect:
            - statusCode: 200
      - post:
          url: "/auth"
          json:
            username: good
            authenticated: true
          headers:
            Accept: application/json
            Content-Type: application/json
          capture:
            - json: "$.access_token"
              as: "access_token"
            - json: "$.refresh_token"
              as: "refresh_token"
          expect:
            - statusCode: 200
      - post:
          url: "/grpc/Echo/echo"
          json:
            message: Hello world
          headers:
            Accept: application/json
            Authorization: "Bearer {{ access_token }}"
          capture:
            - json: "$.message"
              as: message
          expect:
            - statusCode: 200
            - equals:
                - "{{ message }}"
                - "Hello world"

      - get:
          url: "/whoami"
          headers:
            Accept: application/json
            Authorization: "Bearer {{ access_token }}"
          expect:
            - statusCode: 200
      - post:
          url: "/refresh"
          form:
            refresh_token: "{{ refresh_token }}"
            grant_type: refresh_token
          headers:
            Content-Type: application/x-www-form-urlencoded
          capture:
            - json: "$.access_token"
              as: "access_token"
            - json: "$.refresh_token"
              as: "refresh_token"
          expect:
            - statusCode: 200

      - post:
          url: "/grpc/Echo/echo"
          json:
            message: Bye world
          headers:
            Accept: application/json
            Authorization: "Bearer {{ access_token }}"
          capture:
            - json: "$.message"
              as: message
          expect:
            - statusCode: 200
            - equals:
                - "{{ message }}"
                - "Bye world"

      - get:
          url: "/whoami"
          headers:
            Accept: application/json
            Authorization: "Bearer {{ access_token }}"
          expect:
            - statusCode: 200
      - post:
          url: "/logout"
          form:
            token: "{{ refresh_token }}"
            token_type_hint: refresh_token
          headers:
            Content-Type: application/x-www-form-urlencoded
          expect:
            - statusCode: 200
